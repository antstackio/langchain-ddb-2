{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "# from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# from langchain_core.runnables import RunnablePassthrough, RunnableLambda, ConfigurableFieldSpec\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain.memory import ConversationBufferWindowMemory\n",
    "# from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# from langchain_core.chat_history import BaseChatMessageHistory\n",
    "# from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "# from langchain_core.chat_history import BaseChatMessageHistory\n",
    "# from langchain_community.chat_message_histories import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import DynamoDBChatMessageHistory\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda user_id, session_id: DynamoDBChatMessageHistory(\n",
    "        table_name=\"lchain-ddb\",\n",
    "        session_id=session_id,\n",
    "        key={\"user_id\": user_id, \"session_id\": session_id}\n",
    "    ),\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"user_id\",\n",
    "            annotation=str,\n",
    "            name=\"User ID\",\n",
    "            description=\"Unique identifier for the user.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Conversation ID\",\n",
    "            description=\"Unique identifier for the conversation.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# This is where we configure the user and session id\n",
    "config = {\"configurable\": {\"user_id\": \"user_1\", \"session_id\": \"session_1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where we configure the user and session id\n",
    "config = {\"configurable\": {\"user_id\": \"user_1\", \"session_id\": \"session_1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Pradhyumna', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 94, 'total_tokens': 99}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-56c70215-ced1-4015-a386-2f9c96990495-0', usage_metadata={'input_tokens': 94, 'output_tokens': 5, 'total_tokens': 99})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke({\"input\": \"Can you tell me my name?\"}, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and add data to the Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Applications\\miniconda3\\envs\\ML\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction | ðŸ¦œï¸ðŸ”— LangChain\n",
      "\n",
      "Skip to main contentLangChain v0.2 is out! You are currently viewing t\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def bs4_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n",
    "\n",
    "\n",
    "loader = RecursiveUrlLoader(\"https://python.langchain.com/v0.1/\", extractor=bs4_extractor, max_depth=5)\n",
    "all_docs = loader.load()\n",
    "print(all_docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split documents into 6252 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=750, chunk_overlap=100)\n",
    "docs = splitter.transform_documents(all_docs)\n",
    "print(f\"Split documents into {len(docs)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# vectorstore = PineconeVectorStore.from_documents(docs, embeddings, index_name=\"website-data\")\n",
    "vectorstore = PineconeVectorStore(index_name=\"car-data\", embedding=embeddings) # Use this if you already have a Pinecone index\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.search(\"Your Query\",search_type=\"similarity\",k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", streaming=True)\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "template = \"\"\"\n",
    "You are a Car Recommender chatbot. You are helping a user find a car that fits their needs.\n",
    "The context provided to you is a mixture of car specs data as well as reviews and opinions on cars. Make sure you understand this before answering the user's question.\n",
    "You should answer the question based only on the following context provided to you. If you don't have enough information to answer the question, you should say so.\n",
    "\n",
    "Give the output in a nice markdown format.\n",
    "Context:\n",
    "{context}\n",
    "####----####\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", template),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import DynamoDBChatMessageHistory\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    lambda user_id, session_id: DynamoDBChatMessageHistory(\n",
    "        table_name=\"lchain-ddb\",\n",
    "        session_id=session_id,\n",
    "        key={\"user_id\": user_id, \"session_id\": session_id}\n",
    "    ),\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    "    history_factory_config=[\n",
    "                    ConfigurableFieldSpec(\n",
    "                        id=\"user_id\",\n",
    "                        annotation=str,\n",
    "                        name=\"User ID\",\n",
    "                        description=\"Unique identifier for the user.\",\n",
    "                        default=\"\",\n",
    "                        is_shared=True,\n",
    "                    ),\n",
    "                    ConfigurableFieldSpec(\n",
    "                        id=\"session_id\",\n",
    "                        annotation=str,\n",
    "                        name=\"Conversation ID\",\n",
    "                        description=\"Unique identifier for the conversation.\",\n",
    "                        default=\"\",\n",
    "                        is_shared=True,\n",
    "                    ),\n",
    "                ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Good SUVs in Toyota:\\n\\n1. **Toyota Urban Cruiser Hyryder:**\\n   - Co-developed with Maruti, offering a well-rounded practical SUV.\\n   - Features modern design, comfortable interior, and generous equipment list.\\n   - Standout feature is the smooth hybrid technology.\\n   - Lacks in performance compared to turbo-petrol rivals but excels in other areas.\\n   \\n2. **Toyota Fortuner:**\\n   - Popular choice known for its eager performance and off-road capability.\\n   - Considered the default choice in its segment with proven reliability.\\n   - Offers broad, supportive, and comfortable seats with good ventilation.\\n   - Not the most sophisticated SUV in terms of feel but valued for ownership ease and resale value.\\n\\nToyota offers SUVs that cater to different needs, with a focus on practicality, comfort, and reliability, making them popular choices in their respective segments.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"user_id\": \"user2\", \"session_id\": \"session2\"}}\n",
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"In Toyota?\"},\n",
    "    config=config, \n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for answer in conversational_rag_chain.stream({\"input\": \"In Toyota?\"}, config=config):\n",
    "        # Process and stream the output here\n",
    "        for key in answer:\n",
    "                if key == \"answer\":\n",
    "                        print(answer['answer'], end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
